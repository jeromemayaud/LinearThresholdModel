{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import sys\n",
    "from random import random\n",
    "from random import sample\n",
    "from random import choice\n",
    "from random import randint\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import math\n",
    "import sklearn.metrics \n",
    "from scipy import interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Functions from file util.py\n",
    "\n",
    "def load(fname, directed):\n",
    "    import numpy as np\n",
    "    a = np.loadtxt(fname, dtype=np.int)\n",
    "    a[:,:2] -= 1    #convert nodes to 0-indexing\n",
    "    numnodes = len(np.unique(a))\n",
    "    g = np.zeros((numnodes, numnodes))\n",
    "    for i in range(a.shape[0]):\n",
    "        g[a[i, 0], a[i, 1]] = 1\n",
    "        if not directed:\n",
    "            g[a[i, 1], a[i, 0]] = 1\n",
    "    return g\n",
    "\n",
    "def strip(fname):\n",
    "    import networkx as nx\n",
    "    with open(fname) as f:\n",
    "        lines = f.readlines()\n",
    "    lines = [int(x.strip()) for x in lines]\n",
    "    return lines\n",
    "\n",
    "def strip_demo(fname):\n",
    "    import networkx as nx\n",
    "    with open(fname) as f:\n",
    "        lines = [line.split() for line in f]\n",
    "\n",
    "    new_list = []\n",
    "    for sublist in lines:\n",
    "        new_sublist = []\n",
    "        for each in sublist:\n",
    "            each = int(each)\n",
    "            new_sublist.append(each)\n",
    "        new_list.append(new_sublist)\n",
    "\n",
    "    return new_list\n",
    "\n",
    "def load_edgelist(fname):\n",
    "    import numpy as np\n",
    "    import networkx as nx\n",
    "    a = np.loadtxt(fname, dtype=np.int)\n",
    "    #make_ids_contiguous(a)\n",
    "    g = nx.from_edgelist(a)\n",
    "    return g\n",
    "    \n",
    "def make_ids_contiguous(a):\n",
    "    #Input: the array from the original text file input\n",
    "    #Output: the same array, but with the node IDs in a contiguous block starting\n",
    "    #from 0. This lets the node IDs serve as array indices.\n",
    "\n",
    "    import numpy as np\n",
    "    next_id = 0\n",
    "    id_map = {}\n",
    "    for u in np.unique(a[:,:2]):\n",
    "        id_map[u] = next_id\n",
    "        next_id += 1\n",
    "        a[:,:2][a[:,:2] == u] = id_map[u]\n",
    "    return id_map\n",
    "\n",
    "def load_netscience():\n",
    "    import numpy as np\n",
    "    import networkx as nx\n",
    "    a = np.loadtxt('netscience.net', dtype=np.int)\n",
    "    a = a[:, :2]\n",
    "    make_ids_contiguous(a)\n",
    "    return nx.from_edgelist(a)\n",
    "\n",
    "def load_g(netname):\n",
    "    import networkx as nx\n",
    "    import numpy as np\n",
    "    if netname == 'netscience':\n",
    "        g = load_netscience()\n",
    "    elif netname == 'homeless-a':\n",
    "        G = load('a1.txt', directed=False)\n",
    "        g = nx.from_numpy_matrix(G)\n",
    "    elif netname == 'homeless-b':\n",
    "        G = load('b1.txt', directed=False)\n",
    "        g = nx.from_numpy_matrix(G)\n",
    "    elif 'india' in netname:\n",
    "        num = netname.split('-')[1]\n",
    "        G = np.loadtxt('relations/' + num + '-All2.csv', delimiter=',')\n",
    "        g = nx.from_numpy_matrix(G)\n",
    "    elif netname == 'genrel':\n",
    "        g = load_edgelist('genrel.txt')\n",
    "    elif netname == 'SBM-1500-1':\n",
    "        g = nx.read_edgelist('SBM_1500_1_1', nodetype=int)\n",
    "    elif netname == 'SBM-equal_1000_0':\n",
    "        g = nx.read_adjlist('SBM-equal_1000_0', nodetype=int)\n",
    "    elif netname == 'SBM-unequal_1000_0':\n",
    "        g = nx.read_adjlist('SBM-unequal_1000_0', nodetype=int)\n",
    "    elif netname == 'residence':\n",
    "        g = nx.read_edgelist('residence.txt', nodetype=int)\n",
    "    return g\n",
    "\n",
    "#Draw degree histogram with matplotlib.\n",
    "\n",
    "def degree_distribution(fname, list_of_nodes, isolates):\n",
    "    import collections\n",
    "    import matplotlib.pyplot as plt\n",
    "    import networkx as nx\n",
    "\n",
    "    G = load_edgelist(fname)\n",
    "    r = {}\n",
    "    G.add_nodes_from(isolates)\n",
    "\n",
    "    for x in list_of_nodes:\n",
    "        r[x] = G.degree()[x]\n",
    "\n",
    "    degree_sequence=sorted([d for d in r.values()], reverse=True) # degree sequence\n",
    "\n",
    "    degreeCount=collections.Counter(degree_sequence)\n",
    "    deg, cnt = zip(*degreeCount.items())\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.bar(deg, cnt, width=0.80, color='b')\n",
    "\n",
    "    plt.title(\"Degree Histogram\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xlabel(\"Degree\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# how many PLs are they connected to on average? \n",
    "def degree_withPL(fname, list_of_interest, pl_fname, isolates):\n",
    "    import collections\n",
    "    import matplotlib.pyplot as plt\n",
    "    import networkx as nx\n",
    "\n",
    "    G = load_edgelist(fname)\n",
    "    r = {}\n",
    "    G.add_nodes_from(isolates)\n",
    "\n",
    "    pl_list = strip(pl_fname)\n",
    "\n",
    "    for node in list_of_interest:\n",
    "        a = [x for x in G.neighbors(node) if x in pl_list]\n",
    "        r[node] = len(a)\n",
    "\n",
    "    degree_sequence=sorted([d for d in r.values()], reverse=True) # degree sequence\n",
    "\n",
    "    degreeCount=collections.Counter(degree_sequence)\n",
    "    deg, cnt = zip(*degreeCount.items())\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.bar(deg, cnt, width=0.80, color='b')\n",
    "\n",
    "    plt.title(\"Degree Histogram\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xlabel(\"Degree\")\n",
    "    #print(degree_sequence)\n",
    "    plt.show()\n",
    "\n",
    "#makes dictionary of shortest path lengths keyed by PLs\n",
    "#each value is itself another dict, keyed by node number with value path length\n",
    "def social_distance(g, list_of_interest):\n",
    "    import networkx as nx\n",
    "    d={}\n",
    "\n",
    "    for node in list_of_interest:\n",
    "        d[node] = nx.shortest_path_length(g,source=node)\n",
    "    return d\n",
    "\n",
    "def distance_from_PL(fname, pl_fname, convert_fname, nc_fname, isolates_fname):\n",
    "    import networkx as nx\n",
    "    import collections\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    g = load_edgelist(fname)\n",
    "    pl = strip(pl_fname)\n",
    "    convert = strip(convert_fname)\n",
    "    nc = strip(nc_fname)\n",
    "    isolates = strip(isolates_fname)\n",
    "    behavioral = convert + nc\n",
    "    behavioral_without = [x for x in behavioral if x not in isolates]\n",
    "    r = {} #large dictionary, keys are behavioral\n",
    "    print(len(isolates), \"length\")\n",
    "    d = social_distance(g, behavioral_without)\n",
    "    for node in behavioral:\n",
    "        if node in isolates:\n",
    "            r[node] = 0\n",
    "        else:\n",
    "            p = {} #within dictionary, keys are pl, vals are distance, only take minimum in the end\n",
    "            for peer_leader in pl:\n",
    "                try:\n",
    "                    p[peer_leader] = d[node][peer_leader]\n",
    "                except KeyError:\n",
    "                    print(peer_leader, node)\n",
    "                    p[peer_leader]= 5\n",
    "            min_distance = min(p.values())\n",
    "            r[node] = min_distance\n",
    "    print(r)\n",
    "\n",
    "    for node in r.keys():\n",
    "        if r[node]==5:\n",
    "            r[node]=0\n",
    "    c={}\n",
    "    for each in convert:\n",
    "        c[each] = r[each]\n",
    "\n",
    "    distance_convert = sorted([a for a in c.values()], reverse=True)\n",
    "    distance_sequence = sorted([a for a in r.values()], reverse=True) # degree sequence\n",
    "\n",
    "    distanceCount=collections.Counter(distance_sequence)\n",
    "    distanceConvert = collections.Counter(distance_convert)\n",
    "    dis_c, cnt_c = zip(*distanceConvert.items())\n",
    "    dis, cnt = zip(*distanceCount.items())\n",
    "    print(distance_convert, len(distance_convert))\n",
    "    print(distance_sequence, len(distance_sequence))\n",
    "\n",
    "    plt.figure(1)\n",
    "    plt.subplot(211)\n",
    "    plt.bar(dis, cnt, width=0.80, color='b')\n",
    "\n",
    "    plt.title(\"Distance from PL Histogram\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xlabel(\"Distance\")\n",
    "\n",
    "    plt.subplot(212)\n",
    "    plt.bar(dis_c, cnt_c, width=0.80, color = 'r')\n",
    "\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xlabel(\"Distance\")\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "def visualize_set_separate(ICM, p, g, S, converted_list, total, fname, pl_fname, convert_fname, nc_fname, all_nodes):\n",
    "    import networkx as nx\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    pos=nx.spring_layout(g)\n",
    "    total = [x for x in total if x not in S]\n",
    "    converts = [x for x in converted_list if x in total]\n",
    "    converts = [x for x in converts if x not in S]\n",
    "    missing_converts = [x for x in converted_list if x not in total]\n",
    "    missing_converts = [x for x in missing_converts if x not in S]\n",
    "    \n",
    "    n = [x for x in g.nodes() if x not in converted_list]\n",
    "    missing_n = [x for x in n if x not in total]\n",
    "    still_n = [x for x in n if x in total]\n",
    "\n",
    "    all_n = still_n + missing_n\n",
    "    n_colors = ['#565757']*len(still_n) + ['#E0E4E5']*len(missing_n)\n",
    "    all_converts = converts + missing_converts\n",
    "    converts_colors = ['#00C8FF']*len(converts) + ['#C1F0FE']*len(missing_converts)\n",
    "  \n",
    "    print('converts',all_converts)\n",
    "    print('not', all_n)\n",
    "\n",
    "    plt.figure(1)\n",
    "    plt.subplot(211)\n",
    "    nx.draw_networkx_labels(g, pos=pos,with_labels=True,font_size=8)\n",
    "\n",
    "    nx.draw_networkx_nodes(g, pos= pos, nodelist= S, node_color = 'b', node_size=300)\n",
    "    nx.draw_networkx_nodes(g, pos=pos, nodelist=all_converts, node_color = converts_colors, node_size=200)\n",
    "    nx.draw_networkx_nodes(g, pos=pos, nodelist=all_n, node_color = n_colors, node_size = 200)\n",
    "\n",
    "    # edges with missing nodes\n",
    "    missing_edges = g.edges(missing_n + missing_converts)\n",
    "    edge_colors = ['#E3E3E3'] * len(missing_edges)\n",
    "    nx.draw_networkx_edges(g, pos=pos, width = 1.0)\n",
    "    nx.draw_networkx_edges(g, pos=pos, edgelist= missing_edges, width = 0.5, edge_color = edge_colors)\n",
    "    if ICM == True:\n",
    "        plt.title('Simulated ICM with p= %.2f' % p)\n",
    "    else:\n",
    "        plt.title('Simulated TLM')\n",
    "\n",
    "    plt.axis('off')\n",
    "\n",
    "    visualize_set_actual(fname, pl_fname, convert_fname, nc_fname, all_nodes, pos)\n",
    "\n",
    "\n",
    "def visualize_set(g, S, all_nodes):\n",
    "    import networkx as nx\n",
    "    node_color = []\n",
    "    node_size = []\n",
    "#    g = nx.subgraph(g, all_nodes)\n",
    "    for v in g.nodes():\n",
    "        if v in S:\n",
    "            node_color.append('b')\n",
    "            node_size.append(300)\n",
    "        elif v in all_nodes:\n",
    "            node_color.append('y')\n",
    "            node_size.append(100)\n",
    "        else:\n",
    "            node_color.append('k')\n",
    "            node_size.append(20)\n",
    "#    node_size = [300 if v in S else 20 for v in g.nodes()]\n",
    "\n",
    "    nx.draw(g, with_labels=True, node_color = node_color, node_size=node_size)\n",
    "\n",
    "\n",
    "def visualize_set2(g, S, direct, convert):\n",
    "    import networkx as nx\n",
    "\n",
    "    node_color = []\n",
    "    node_size = []\n",
    "#    g = nx.subgraph(g, all_nodes)\n",
    "    for v in g.nodes():\n",
    "        if v in S:\n",
    "            node_color.append('blue')\n",
    "            node_size.append(300)  \n",
    "        elif v in direct:\n",
    "            if v in convert:\n",
    "                node_color.append('green')\n",
    "                node_size.append(200)\n",
    "            else:\n",
    "                node_color.append('yellowgreen')\n",
    "                node_size.append(200)\n",
    "        elif v in convert:\n",
    "            node_color.append('red')\n",
    "            node_size.append(200)\n",
    "        else:\n",
    "            node_color.append('k')\n",
    "            node_size.append(20)\n",
    "#    node_size = [300 if v in S else 20 for v in g.nodes()]\n",
    "    nx.draw(g, with_labels=True, node_color = node_color, node_size=node_size)\n",
    "\n",
    "def visualize_set_actual(fname, pl_fname, convert_fname, nc_fname, all_nodes, pos, min_color, max_color):\n",
    "    import networkx as nx\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    g = load_edgelist(fname)\n",
    "    pl = strip(pl_fname)\n",
    "    convert = strip(convert_fname)\n",
    "    nc = strip(nc_fname)\n",
    "    behavioral = convert + nc\n",
    "\n",
    "    isolates = [x for x in all_nodes if x not in g.nodes()]\n",
    "    print(isolates)\n",
    "    g.add_nodes_from(isolates)\n",
    "    # pos=nx.spring_layout(g)\n",
    "\n",
    "    n = [x for x in all_nodes if x in nc]\n",
    "    converts = [x for x in all_nodes if x in convert]\n",
    "    missing = [x for x in all_nodes if x not in behavioral]\n",
    "    missing = [x for x in missing if x not in pl]\n",
    "  \n",
    "\n",
    "    full = converts + n\n",
    "    full_colors = [max_color]*len(converts)+[min_color+0.5]*len(n)\n",
    "    n_colors = [min_color]*len(n)\n",
    "    converts_colors = [max_color]*len(converts)\n",
    "    missing_colors = ['#F0F2F4']*len(missing)\n",
    "    print(\"v_min\", min_color)\n",
    "    print(\"v_max\", max_color)\n",
    "\n",
    "    blah = [max_color] * len(pl)\n",
    "    # plt.subplot(311)\n",
    "    plt.figure(1)\n",
    "    nx.draw_networkx_labels(g,pos=pos,with_labels=True,font_size=8)\n",
    "\n",
    "    nx.draw_networkx_nodes(g, pos= pos, nodelist= pl, node_color = 'r', node_size=300)\n",
    "    nx.draw_networkx_nodes(g, pos=pos, nodelist=full, node_color = full_colors, cmap= plt.cm.PiYG,node_size=200)\n",
    "    nx.draw_networkx_nodes(g, pos=pos, nodelist=missing, node_color = missing_colors, node_size = 200)\n",
    "\n",
    "    # edges with missing nodes\n",
    "    missing_edges = g.edges(missing)\n",
    "    edge_colors = ['#E3E3E3'] * len(missing_edges)\n",
    "    nx.draw_networkx_edges(g, pos=pos, width = 1.0)\n",
    "    nx.draw_networkx_edges(g, pos=pos, edgelist= missing_edges, width = 0.5, edge_color = edge_colors)\n",
    "    \n",
    "    plt.title('Actual Network')\n",
    "    plt.axis('off')\n",
    "# sum up the neighborhood overlap for the entire PL set\n",
    "# for each node, sum of overlap with every other PL nodee\n",
    "def overlap(fname,pl_fname):\n",
    "    import networkx as nx\n",
    "\n",
    "    g = load_edgelist(fname)\n",
    "    list_of_nodes = strip(pl_fname)\n",
    "    list_of_nodes_2 = list_of_nodes[1:]\n",
    "    overlap = []\n",
    "    total = 0\n",
    "\n",
    "    for node in list_of_nodes:\n",
    "        for next in list_of_nodes_2:\n",
    "            neighborhood = g.neighbors(next)\n",
    "            neighborhood.append(next)\n",
    "            overlap  = [x for x in g.neighbors(node) if x in neighborhood]\n",
    "            print(overlap)\n",
    "            node_neighbor = g.neighbors(node)\n",
    "            node_neighbor.append(node)\n",
    "            overlap = len(overlap)/float(len(node_neighbor+neighborhood))\n",
    "            total = total + overlap\n",
    "        if list_of_nodes_2: \n",
    "            list_of_nodes_2.pop(0)\n",
    "\n",
    "    return total\n",
    "            \n",
    "\n",
    "def overlap2(fname,pl_list):\n",
    "    import networkx as nx\n",
    "\n",
    "    g = load_edgelist(fname)\n",
    "    pl_list2 = pl_list[1:]\n",
    "    overlap = []\n",
    "    total = 0\n",
    "\n",
    "    for node in pl_list:\n",
    "        for next in pl_list2:\n",
    "            neighborhood = g.neighbors(next)\n",
    "            neighborhood.append(next)\n",
    "            overlap  = [x for x in g.neighbors(node) if x in neighborhood]\n",
    "            print(overlap)\n",
    "            node_neighbor = g.neighbors(node)\n",
    "            node_neighbor.append(node)\n",
    "            overlap = len(overlap)/float(len(node_neighbor+neighborhood))\n",
    "            total = total + overlap\n",
    "        if pl_list2: \n",
    "            pl_list2.pop(0)\n",
    "\n",
    "    return total\n",
    "\n",
    "def overlap_onlyPL(fname,pl_list):\n",
    "    import networkx as nx\n",
    "\n",
    "    g = load_edgelist(fname)\n",
    "    total = 0\n",
    "    overlap = []\n",
    "\n",
    "    for node in pl_list:\n",
    "        overlap = [x for x in g.neighbors(node) if x in pl_list]\n",
    "        print(overlap)\n",
    "        overlap = len(overlap)/float(len(g.neighbors(node)))\n",
    "        print(overlap)\n",
    "        total = total + overlap\n",
    "    return total\n",
    "\n",
    "def redundancy(fname,pl_fname):\n",
    "    import networkx as nx\n",
    "\n",
    "    g= load_edgelist(fname)\n",
    "    pl = strip(pl_fname)\n",
    "    r = []\n",
    "    penalty = []\n",
    "    for each in pl:\n",
    "        remove_this = [] \n",
    "        print(\"pl in question\", each)\n",
    "        ego = nx.ego_graph(g, each, radius = 1)\n",
    "        n = len(ego.nodes())-1\n",
    "        t = nx.edges(ego)\n",
    "        print(\"edges\", t)\n",
    "        for node1, node2 in t:\n",
    "            print(\"check\", node1, node2)\n",
    "            if node1 == each or node2 == each:\n",
    "                remove_this.append((node1, node2))\n",
    "                print(\"removed\", node1, node2, each)\n",
    "        print(\"removed this\", remove_this)\n",
    "        t = [x for x in t if x not in remove_this]     \n",
    "        print(\"t\", len(t))\n",
    "        print(\"n\", n)\n",
    "        r.append(n-2*len(t)/float(n))\n",
    "        penalty.append(2*len(t)/float(n))\n",
    "\n",
    "    return penalty, sum(penalty), sum(r)\n",
    "\n",
    "\n",
    "def percent_withPL(fname,pl_list):\n",
    "    import networkx as nx\n",
    "\n",
    "    g = load_edgelist(fname)\n",
    "    total = 0\n",
    "    overlap = []\n",
    "    pl_list2  = pl_list[1:]\n",
    "    total_degree = 0\n",
    "\n",
    "    for node in pl_list:\n",
    "        overlap = [x for x in g.neighbors(node) if x in pl_list2]\n",
    "        overlap = len(overlap)\n",
    "        # overlap = len(overlap)/float(len(g.neighbors(node)))\n",
    "        print(overlap)\n",
    "        skip = [x for x in pl_list if x not in pl_list2]\n",
    "        print(skip)\n",
    "        degree = len([x for x in g.neighbors(node) if x not in skip])\n",
    "        print(degree)\n",
    "        total_degree = total_degree + degree\n",
    "        total = total + overlap\n",
    "        if pl_list2: pl_list2.pop(0)\n",
    "\n",
    "    \n",
    "    return total/float(total_degree)\n",
    "\n",
    "def spheres(fname, pl_fname, convert_fname, nc_fname):\n",
    "    import networkx as nx\n",
    "\n",
    "    g = load_edgelist(fname)\n",
    "    pl_list = strip(pl_fname)\n",
    "    convert = strip(convert_fname)\n",
    "    nc = strip(nc_fname)\n",
    "    total = nc+convert\n",
    "    pl_list2 = pl_list[:]\n",
    "    number_converted=0\n",
    "    c={}\n",
    "    n={}\n",
    "\n",
    "    for node in pl_list:\n",
    "        c[node] = [x for x in pl_list2 if x in g.neighbors(node)]\n",
    "        n[node] = [x for x in pl_list2 if x not in g.neighbors(node)]\n",
    "\n",
    "    s_c = {} \n",
    "    s_n = {}\n",
    "\n",
    "    for key in c.keys():\n",
    "        list_of_percent = []\n",
    "        list_of_percent2 = []\n",
    "        converted_original = [x for x in g.neighbors(key) if x in convert]\n",
    "        only_original = [x for x in g.neighbors(key) if x not in pl_list]\n",
    "        only_original = [x for x in only_original if x in total]\n",
    "\n",
    "        by_itself_convert = len(converted_original)/float(len(only_original))\n",
    "        list_of_percent.append(by_itself_convert)\n",
    "        list_of_percent2.append(by_itself_convert)\n",
    "        for each in c[key]:\n",
    "            converted = [x for x in g.neighbors(each) if x in convert]\n",
    "            converted = [x for x in converted if x not in pl_list]\n",
    "            top = len(set(converted+converted_original))\n",
    "            only = [x for x in g.neighbors(each) if x not in pl_list]\n",
    "            only = [x for x in only if x in total]\n",
    "            denom = len(set(only+only_original))\n",
    "            if denom ==0:\n",
    "                number_converted = 'denom 0'\n",
    "            else:\n",
    "                number_converted = top/float(denom)\n",
    "            list_of_percent.append((each,number_converted, 'out of = %d' % denom))\n",
    "        s_c[key] = list_of_percent\n",
    "        for each in n[key]:\n",
    "            if each == key:\n",
    "                continue\n",
    "            converted2 = [x for x in g.neighbors(each) if x in convert]\n",
    "            converted2 = [x for x in converted2 if x not in pl_list]\n",
    "            top = len(set(converted2 + converted_original))\n",
    "            only = [x for x in g.neighbors(each) if x not in pl_list]\n",
    "            only = [x for x in only if x in total]\n",
    "            denom = len(set(only+only_original))\n",
    "            if denom == 0:\n",
    "                number_converted2 = 'denom 0'\n",
    "            else:\n",
    "                number_converted2 = top/float(denom)\n",
    "            try:\n",
    "                path = nx.shortest_path_length(g,source=key,target=each)\n",
    "            except nx.exception.NetworkXNoPath:\n",
    "                path = 'None'\n",
    "            list_of_percent2.append((each, path,number_converted2, 'out of = %d' % denom))\n",
    "        s_n[key] = list_of_percent2\n",
    "\n",
    "    print(s_c)\n",
    "    print(s_n)\n",
    "\n",
    "\n",
    "def spheres_intersect(fname,pl_fname,convert_fname,nc_fname):\n",
    "    import networkx as nx\n",
    "\n",
    "    g = load_edgelist(fname)\n",
    "    pl = strip(pl_fname)\n",
    "    convert = strip(convert_fname)\n",
    "    nc = strip(nc_fname)\n",
    "    total = convert + nc\n",
    "    only_neighbors_e = []\n",
    "    only_neighbors_c = []\n",
    "    counter_edge = 0\n",
    "    counter_both = 0\n",
    "\n",
    "    for each in total:\n",
    "        try:\n",
    "            pl_neighbors = [x for x in g.neighbors(each) if x in pl]\n",
    "        except nx.exception.NetworkXError:\n",
    "            continue\n",
    "        number_pl_neighbors = len(pl_neighbors)\n",
    "        if each in convert: \n",
    "            converted = 1\n",
    "        else:\n",
    "            converted = 0\n",
    "        edge = 'no edge'\n",
    "        if len(pl_neighbors):\n",
    "            edge = len(pl_neighbors)\n",
    "            counter_edge += 1\n",
    "            if each in convert:\n",
    "                counter_both += 1\n",
    "            only_neighbors_e.append(edge)\n",
    "            only_neighbors_c.append(converted)\n",
    "    baseline_percent = len(convert)/float(len(total))\n",
    "    edge_percent = counter_both/float(counter_edge)\n",
    "\n",
    "    print(baseline_percent)\n",
    "    print(edge_percent)\n",
    "    print(only_neighbors_e)\n",
    "    print(only_neighbors_c)\n",
    "\n",
    "# returns list of all neighbors of a set of nodes \n",
    "def neighbors(g, S):\n",
    "    import networkx as nx\n",
    "\n",
    "    set_of_nbs = set()\n",
    "    for node in S:\n",
    "        for nb in g.neighbors(node):\n",
    "            set_of_nbs.add(nb)\n",
    "    return list(set_of_nbs)\n",
    "\n",
    "def only_behavioral(direct, convert, nc):\n",
    "    import networkx as nx\n",
    "\n",
    "    behavioral = []\n",
    "    for v in direct:\n",
    "        if (v in convert) or (v in nc):\n",
    "            behavioral.append(v)\n",
    "    return behavioral\n",
    "\n",
    "def visualize_communities(fname, convert_fname, nc_fname, pl_fname):\n",
    "    import networkx as nx\n",
    "    import community\n",
    "    import numpy as np\n",
    "    import random\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    g = load_edgelist(fname)\n",
    "    convert = strip(convert_fname)\n",
    "    nc = strip(nc_fname)\n",
    "    S = strip(pl_fname)\n",
    "\n",
    "    total = convert + nc\n",
    "\n",
    "    isolates = [x for x in total if x not in g.nodes()]\n",
    "    g.add_nodes_from(isolates)\n",
    "\n",
    "    part = community.best_partition(g)\n",
    "    copy_part = part\n",
    "    print(part)\n",
    "    part = [part[x] for x in g.nodes()]\n",
    "    print(part)\n",
    "    com_names = np.unique(part)\n",
    "    communities = []\n",
    "    node_contig = range(len(g.nodes()))\n",
    "\n",
    "\n",
    "    for i,c in enumerate(com_names):\n",
    "        print(com_names)\n",
    "        communities.append([])\n",
    "        print(communities, i)\n",
    "        print(communities[0], \"communities 0\")\n",
    "        print(communities)\n",
    "        test = [x for x in node_contig if part[x]==c]\n",
    "        print(test)\n",
    "        communities[i].extend([x for x in node_contig if part[x] == c])\n",
    "    node_color = part\n",
    "    pos = nx.layout.spring_layout(g, k=0.1)\n",
    "\n",
    "    labels={}\n",
    "    for node in S:\n",
    "        labels[node] = node\n",
    "\n",
    "    nx.draw(g, with_labels=False, node_color=node_color, pos=pos, node_size=50)\n",
    "    nx.draw_networkx_labels(g,pos,labels,font_size=12,font_color='black')\n",
    "\n",
    "    print(part)\n",
    "    print(copy_part)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def write_for_metis(g, fname):\n",
    "    with open(fname, 'w') as f:\n",
    "        f.write(str(g.number_of_nodes()) + ' ' + str(g.number_of_edges()) + '\\n')\n",
    "        for v in g.nodes():\n",
    "            for u in g.neighbors(v):\n",
    "                f.write(str(u+1) + ' ')\n",
    "            f.write('\\n')\n",
    "  \n",
    "def recursive_split(g):\n",
    "    if len(g) == 1:\n",
    "        return [g.nodes()[0]]\n",
    "    return [recursive_split(g.subgraph(g.nodes()[:len(g)/2])), recursive_split(g.subgraph(g.nodes()[len(g)/2:]))]    \n",
    "      \n",
    "def recursive_partition(g, fname):\n",
    "    print('CALL', len(g))\n",
    "    import subprocess\n",
    "    import numpy as np\n",
    "    import networkx as nx\n",
    "    if len(g) == 1:\n",
    "        return [g.nodes()[0]]\n",
    "    if g.number_of_edges() == 0:\n",
    "        return recursive_split(g)\n",
    "\n",
    "    contig_g = nx.from_numpy_matrix(nx.to_numpy_matrix(g))\n",
    "    write_for_metis(contig_g, fname)\n",
    "    subprocess.call(['rm', fname + '.part.2'])\n",
    "    subprocess.call(['./gpmetis', fname, '2'])\n",
    "    labels = np.loadtxt(fname + '.part.2')\n",
    "    nodes = np.array(g.nodes())\n",
    "    print(len(nodes), len(labels))\n",
    "    part1 = nodes[labels == 0]\n",
    "    part2 = nodes[labels == 1]\n",
    "    if len(part1) == 0:\n",
    "        return recursive_split(g.subgraph(part2))      \n",
    "    if len(part2) == 0:\n",
    "        return recursive_split(g.subgraph(part1))\n",
    "    return [recursive_partition(g.subgraph(part1), fname), recursive_partition(g.subgraph(part2), fname)]\n",
    "\n",
    "def greedy_icm(g, budget, rr_sets = None, start_S = None):\n",
    "    from rr_icm import make_rr_sets_cython, eval_node_rr\n",
    "    import heapq\n",
    "    num_nodes = len(g)\n",
    "    allowed_nodes = range(num_nodes)\n",
    "    if rr_sets == None:\n",
    "        rr_sets = make_rr_sets_cython(g, 500, range(num_nodes))\n",
    "    if start_S == None:\n",
    "        S = set()\n",
    "    else:\n",
    "        S = start_S\n",
    "    upper_bounds = [(-eval_node_rr(u, S, num_nodes, rr_sets), u) for u in allowed_nodes]    \n",
    "    heapq.heapify(upper_bounds)\n",
    "    starting_objective = 0\n",
    "    #greedy selection of K nodes\n",
    "    while len(S) < budget:\n",
    "        val, u = heapq.heappop(upper_bounds)\n",
    "        new_total = eval_node_rr(u, S, num_nodes, rr_sets)\n",
    "        new_val =  new_total - starting_objective\n",
    "        #lazy evaluation of marginal gains: just check if beats the next highest upper bound\n",
    "        if new_val >= -upper_bounds[0][0] - 0.1:\n",
    "            S.add(u)\n",
    "            starting_objective = new_total\n",
    "        else:\n",
    "            heapq.heappush(upper_bounds, (-new_val, u))\n",
    "    return S, starting_objective\n",
    "\n",
    "def greedy(items, budget, f):\n",
    "    '''\n",
    "    Generic greedy algorithm to select budget number of items to maximize f.\n",
    "    \n",
    "    Employs lazy evaluation of marginal gains, which is only correct when f is submodular.\n",
    "    '''\n",
    "    import heapq\n",
    "    upper_bounds = [(-f(set([u])), u) for u in items]    \n",
    "    heapq.heapify(upper_bounds)\n",
    "    starting_objective = f(set())\n",
    "    S  = set()\n",
    "    #greedy selection of K nodes\n",
    "    while len(S) < budget:\n",
    "        val, u = heapq.heappop(upper_bounds)\n",
    "        new_total = f(S.union(set([u])))\n",
    "        new_val =  new_total - starting_objective\n",
    "        #lazy evaluation of marginal gains: just check if beats the next highest upper bound\n",
    "        if new_val >= -upper_bounds[0][0] - 0.1:\n",
    "            S.add(u)\n",
    "            starting_objective = new_total\n",
    "        else:\n",
    "            heapq.heappush(upper_bounds, (-new_val, u))\n",
    "    return S, starting_objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############ MAIN FUNCTIONS FOR RUNNING THE LINEAR THRESHOLD MODEL ############\n",
    "\n",
    "#This function simulates how diffusion occurs according to the LTM, a standard information diffusion model\n",
    "def simulate_LTM(g, pl, time_limit):\n",
    "\tconverted_list = pl[:]\n",
    "\tthreshold = {}\n",
    "\n",
    "\tfor node in g.nodes():\n",
    "\t\tthreshold[node] = random()\n",
    "\tfor t in range(time_limit):\n",
    "\t\tconverted_list1 = converted_list[:]\n",
    "\t\tfor node in g.nodes():\n",
    "\t\t\ttotal_weight = 0\n",
    "\t\t\tif g.degree()[node]:\n",
    "\t\t\t\tweight = 1/float(g.degree()[node])\n",
    "\t\t\telse:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tfor each in g.neighbors(node):\n",
    "\t\t\t\tif each in converted_list:\n",
    "\t\t\t\t\ttotal_weight = total_weight + weight\n",
    "\t\t\tif total_weight > threshold[node]:\n",
    "\t\t\t\tconverted_list.append(node)\n",
    "\t\tif set(converted_list1) == set(converted_list):\n",
    "\t\t\t#print('set', t)\n",
    "\t\t\tbreak\n",
    "\treturn converted_list #This returns all the nodes in the network that have been activated/converted in the diffusion process\n",
    "\n",
    "#This function runs the LTM with the necessary parameters\n",
    "def main(g, all_nodes, pl, convert, nc, time_limit, number): \n",
    "    r = {}\n",
    "    new_range = np.linspace(0,1,100).tolist()\n",
    "    \n",
    "    all_nodes1 = []\n",
    "    for sublist in all_nodes:\n",
    "        all_nodes1.append(sublist[0])\n",
    "        \n",
    "    all_nodes_except_PL = [x for x in all_nodes1 if x not in pl]\n",
    "\t\n",
    "    for node in all_nodes_except_PL:\n",
    "        r[node] = 0\n",
    "\n",
    "    #Calculate the False Positive Rates and True Positive Rates by comparing predicted vs observed node activations\n",
    "    isolates = [x for x in all_nodes1 if x not in g.nodes()]\n",
    "    g.add_nodes_from(isolates)\n",
    "    binary_index = []\n",
    "    for converted in all_nodes_except_PL:\n",
    "        if converted in convert:\n",
    "            binary_index.append(1)\n",
    "        elif converted in nc:\n",
    "            binary_index.append(0)\n",
    "        else:\n",
    "            binary_index.append(-1)\n",
    "    FPR_total = []\n",
    "    TPR_total = []\n",
    "\n",
    "    sensitivity, specificity = 0, 0\n",
    "    running_s = []\n",
    "    running_p = []\n",
    "    \t\t\t\n",
    "    running_binary_sim = [0]*len(all_nodes_except_PL)\n",
    "    number_of_predicted_positives = np.zeros(number)\n",
    "    \n",
    "    for i in range(number):\n",
    "        converted_list = simulate_LTM(g, pl, time_limit)\n",
    "        converted_list = [x for x in converted_list if x not in pl]\n",
    "        binary_sim = []\n",
    "        for each in all_nodes_except_PL:\n",
    "            if each in converted_list:\n",
    "                binary_sim.append(1)\n",
    "            else:\n",
    "                binary_sim.append(0)\n",
    "        running_binary_sim = [sum(x) for x in zip(binary_sim, running_binary_sim)]\n",
    "        number_of_predicted_positives[i] = sum(binary_sim)\n",
    "      \n",
    "    running_binary_sim = [x/float(number) for x in running_binary_sim]\n",
    "    \n",
    "    running_binary_sim = np.array(running_binary_sim)\n",
    "    binary_index = np.array(binary_index)\n",
    "    have_data = np.where(binary_index != -1)[0]\n",
    "    \n",
    "    max_auc_sklearn = sklearn.metrics.roc_auc_score(binary_index[have_data], running_binary_sim[have_data])\n",
    "    roc_curve = sklearn.metrics.roc_curve(binary_index[have_data], running_binary_sim[have_data])\n",
    "    \n",
    "    avg_predicted_positives = np.round(np.mean(number_of_predicted_positives))\n",
    "    observed_positives = np.size(np.where(binary_index == 1))\n",
    "        \n",
    "    print('AUC:', max_auc_sklearn)\n",
    "    \n",
    "    return max_auc_sklearn, avg_predicted_positives, observed_positives, roc_curve\n",
    "\n",
    "#This function runs 'main' function multiple times to derive average performance scores\n",
    "def multiple_runs(g, all_nodes, pl, convert, nc, time_limit, number, number_of_simulations):\n",
    "    \n",
    "    running_auc_results = []\n",
    "    avg_predicted_positives_list = []\n",
    "    observed_positives_list = []\n",
    "    ratio_predicted_to_observed_positives_list = []\n",
    "    running_fpr_results = []\n",
    "    running_tpr_results = []\n",
    "    \n",
    "    for i in range (number_of_simulations):\n",
    "        max_auc_sklearn, avg_predicted_positives, observed_positives, roc_curve = main(g, all_nodes, pl, convert, nc, time_limit, number)\n",
    "        \n",
    "        ratio_predicted_to_observed_positives = avg_predicted_positives/observed_positives\n",
    "        \n",
    "        print('Ratio of predicted to observed positives:', ratio_predicted_to_observed_positives)\n",
    "        \n",
    "        running_auc_results.append(max_auc_sklearn)\n",
    "        avg_predicted_positives_list.append(avg_predicted_positives)\n",
    "        observed_positives_list.append(observed_positives)\n",
    "        ratio_predicted_to_observed_positives_list.append(ratio_predicted_to_observed_positives)\n",
    "        running_fpr_results.append(roc_curve[0])\n",
    "        running_tpr_results.append(roc_curve[1])\n",
    "        \n",
    "        print(\"*****\", i+1, \" simulations run *****\")\n",
    "\n",
    "    auc_mean = np.mean(running_auc_results)\n",
    "    auc_stdev = np.std(running_auc_results)\n",
    "    avg_predicted_positives_mean = np.mean(avg_predicted_positives_list)\n",
    "    observed_positives_mean = np.mean(observed_positives_list)\n",
    "    ratio_predicted_to_observed_positives_mean = np.mean(ratio_predicted_to_observed_positives_list)\n",
    "    ratio_predicted_to_observed_positives_std = np.std(ratio_predicted_to_observed_positives_list)\n",
    "\n",
    "    #Find macro-average of ROC curves\n",
    "    mean_fpr = np.unique(np.concatenate([running_fpr_results[i] for i in range(number_of_simulations)])) #Aggregate all false positive rates\n",
    "    mean_tpr = np.zeros_like(mean_fpr) #Interpolate all ROC curves at these points\n",
    "    \n",
    "    temporary_tpr = np.zeros_like(mean_fpr) \n",
    "    temporary_count = np.zeros_like(mean_fpr)\n",
    "    \n",
    "    for k in range(np.size(mean_fpr)):\n",
    "        fpr_value = mean_fpr[k]\n",
    "        for i in range(number_of_simulations):\n",
    "            temporary_tpr[k] += np.sum(running_tpr_results[i][np.where(running_fpr_results[i] == fpr_value)])\n",
    "            temporary_count[k] += np.size(np.where(running_fpr_results[i] == fpr_value))\n",
    "    mean_tpr = temporary_tpr/temporary_count\n",
    "\n",
    "   \n",
    "    return auc_mean, auc_stdev, ratio_predicted_to_observed_positives_mean, ratio_predicted_to_observed_positives_std, avg_predicted_positives_mean, observed_positives_mean, running_fpr_results, running_tpr_results, mean_fpr, mean_tpr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.153153153153\n",
      "Ratio of predicted to observed positives: 0.540540540541\n",
      "***** 1  simulations run *****\n",
      "AUC: 0.117117117117\n",
      "Ratio of predicted to observed positives: 0.567567567568\n",
      "***** 2  simulations run *****\n",
      "AUC: 0.121621621622\n",
      "Ratio of predicted to observed positives: 0.567567567568\n",
      "***** 3  simulations run *****\n",
      "AUC: 0.126126126126\n",
      "Ratio of predicted to observed positives: 0.567567567568\n",
      "***** 4  simulations run *****\n",
      "AUC: 0.198198198198\n",
      "Ratio of predicted to observed positives: 0.540540540541\n",
      "***** 5  simulations run *****\n",
      "AUC: 0.193693693694\n",
      "Ratio of predicted to observed positives: 0.567567567568\n",
      "***** 6  simulations run *****\n",
      "AUC: 0.153153153153\n",
      "Ratio of predicted to observed positives: 0.567567567568\n",
      "***** 7  simulations run *****\n",
      "AUC: 0.117117117117\n",
      "Ratio of predicted to observed positives: 0.567567567568\n",
      "***** 8  simulations run *****\n",
      "AUC: 0.175675675676\n",
      "Ratio of predicted to observed positives: 0.594594594595\n",
      "***** 9  simulations run *****\n",
      "AUC: 0.162162162162\n",
      "Ratio of predicted to observed positives: 0.567567567568\n",
      "***** 10  simulations run *****\n",
      "0.151801801802\n"
     ]
    }
   ],
   "source": [
    "###### ACTUALLY RUN THE MODEL ########\n",
    "#File directories\n",
    "fname = 'xxxx/MFP_GSN_undirected_trimmed_Baseline.txt'\n",
    "pl_fname = 'xxxx/MFP_PL.txt'\n",
    "convert_fname = 'xxxx/MFP_ConfirmedConversations_3M.txt'\n",
    "nc_fname = 'xxxx/MFP_nc_1M.txt'\n",
    "all_nodes_fname = 'xxx/MFP_GSN_untrimmed_allnodes.txt'\n",
    "\n",
    "#Descriptions of files\n",
    "#'fname' is a list of the node IDs of all members in your social network\n",
    "#'pl_fname' is a list of the node IDs of the Peer Leaders in your network\n",
    "#'convert_fname' is a list of the node IDs of the members who were observed to be converted\n",
    "#'nc_fname' is a list of the node IDs of the members who were NOT observed to be converted\n",
    "#'all_nodes_fname' is a complete list of all named and unnamed node IDs in the network\n",
    "\n",
    "#Load edge lists etc.\n",
    "g = load_edgelist(fname) #Load up edge list as a graph edge list\n",
    "pl = strip(pl_fname) #List of peer leaders\n",
    "convert = strip(convert_fname)\n",
    "nc = strip(nc_fname)\n",
    "all_nodes = strip_demo(all_nodes_fname)\n",
    "\n",
    "#Parameters to change in the model:\n",
    "time_limit = 100 #Time limit after which the simulation ends (usually set at 100)\n",
    "number = 100 # Number of times each simulation is iterated (usually set at 100)\n",
    "number_of_simulations = 10 #Number of separate simulations you want to average over (usually set at 10)\n",
    "\n",
    "#This line executes the interrelated functions to produce performance metrics of the LTM simulations\n",
    "auc_mean, auc_stdev, ratio_predicted_to_observed_positives_mean, ratio_predicted_to_observed_positives_std, avg_predicted_positives_mean, observed_positives_mean, running_fpr_results, running_tpr_results, mean_fpr, mean_tpr = multiple_runs(g, all_nodes, pl, convert, nc, 100, 100, 10)\n",
    "\n",
    "print(auc_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
